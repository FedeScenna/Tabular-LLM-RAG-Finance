{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data/database.db\")\n",
    "tickers = pd.read_sql_query(\"SELECT distinct Security FROM master_ticker\", conn)[\"Security\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keys.yaml\") as keys:\n",
    "    try:\n",
    "        api_keys = yaml.safe_load(keys)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\feder\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:130: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.llms.huggingface_text_gen_inference import (  # type: ignore[import-not-found]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = api_keys[\"hf_model\"]\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"data/wikipedia/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:27<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_use_double_quant=True,\n",
    " bnb_4bit_quant_type=\"nf4\",\n",
    " bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_33244\\1025737063.py:15: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_33244\\1025737063.py:35: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_33244\\1025737063.py:52: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm_chain.run({\"context\": context, \"question\": question})\n",
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:479: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "The AES Corporation is an American utility and power generation company. It owns and operates\n",
      "power plants, which it uses to generate and sell electricity to end users and intermediaries like\n",
      "utilities and industrial facilities. AES, headquartered in Arlington, Virginia, is one of the world's\n",
      "leading power companies, generating and distributing electric power in 15 countries[2] and\n",
      "employing 10,500 people worldwide. AES Corporation is a global Fortune 500 power company.[3]\n",
      "AES Ranks in the Top Ten of Fast Company's 2022 Best Workplaces for Innovators.[4]\n",
      "The company was founded on January 28, 1981, as Applied Energy Services[5] by Roger Sant and\n",
      "Dennis Bakke, two appointees of the Federal Energy Administration under president Richard Nixon.\n",
      "The company was initially a consulting firm; it became AES Corporation, which went public in 1991.\n",
      "Sant was chairman, CEO, and president and Bakke was executive vice president until assuming the\n",
      "position of president in 1987. Bakke later became the company's CEO in 1994, serving for eight\n",
      "years until his resignation in 2002 in the midst of a liquidity crisis that followed the collapse of the\n",
      "energy giant Enron.[6][7] Sant remained as executive chairman until 2003 and as a member of the\n",
      "board until 2006. Paul Hanrahan was appointed President and CEO and remained for ten years,\n",
      "overseeing the stabilization of the company. Until the early 2000's, the company followed\n",
      "self-management, delegating much responsibility to ordinary employees.[8] In 2012, Hanrahan\n",
      "resigned his position as president and CEO of the company, and he was succeeded by Andres\n",
      "Gluski. As CEO, Gluski has implemented a strategy of reducing the number of countries in which\n",
      "AES does business, from 28 to 16, for the purpose of consolidating operations and reducing costs.\n",
      "Additionally, he began a program of reducing the company's total carbon emission intensity.\n",
      "Bakke and Sant oversaw much of AES's initial global expansion, building power plants in 29\n",
      "countries and expanding its staff from 1,400 to 32,000 employees, and also instilled a system of\n",
      "decentralized management that emphasized social responsibility above profit. In recent years, AES\n",
      "has signaled a commitment to providing its consumers and clients with renewable forms of energy,\n",
      "and its operations across the world increasingly have focused on the construction and provision of\n",
      "solar and wind-based energy storage systems.\n",
      "\n",
      "AES acquired the assets of Indianapolis Power & Light, Ipalco, in 2000.[9] In February, 2021,\n",
      "Indianapolis Power & Light rebranded as AES Indiana.[10]\n",
      "AES acquired the Chilean-based subsidiary Gener in 2000 and acquired DPL Inc., then known as\n",
      "Dayton Power & Light, in 2011.[11] The company was rebranded as AES Ohio in February,\n",
      "2021.[12]\n",
      "In fiscal 2015 AES's total revenue was $15 billion.[13]\n",
      "In 2018, AES acquired the subsidiary sPower.\n",
      "In December, 2021, AES acquired Community Energy Solar (Community Energy) to help deliver 4\n",
      "GW of renewables in the U.S.[14]\n",
      "Launched during January 2018, Fluence is a joint venture between AES Energy Storage and\n",
      "Siemens that is focused on the development of, and expansion of energy storage technologies and\n",
      "services. Chaired by former AES vice-president for energy storage platforms Stephen Coughlin, and\n",
      "headquartered in Washington D.C., Fluence aims to implement AES's extensive research into the\n",
      "potential of lithium-ion powered energy sources by relying upon Siemens' expansive global\n",
      "presence in the industrial sector; for the purpose of addressing the rapidly rising demand for clean\n",
      "energy technologies.[15] Fluence has been deployed in 16 countries, with major projects including\n",
      "the following:[16]\n",
      "Fluence is expected to employ three different types of grid technology. WP:Crystal\n",
      "i):SIESTORAGE: An electrical energy storage system fueled predominantly by wind and solar\n",
      "energy. Siestorage relies upon the closed-loop controls and pulse modulation built into its\n",
      "semiconductors, in order to provide consumers with increased dependability.[20]\n",
      "ii):Advancion: A storage system that is made up of several small, modular nodes and powered by\n",
      "lithium-ion batteries, which enables the Advancion energy store system to provide consumers with a\n",
      "heightened degree of consistency in performance.[21]\n",
      "iii): Sunflux Energy: Announced in January 2018, Sunflex was developed for the purpose of\n",
      "expanding upon the possibilities offered by photovoltaic solar energy. This technology is built to\n",
      "capture energy during peak solar hours in order to expand energy delivery.[22][23]\n",
      "\n",
      "Arista Networks, Inc. (formerly Arastra)[3] is an American computer networking company\n",
      "headquartered in Santa Clara, California. The company designs and sells multilayer network\n",
      "switches to deliver software-defined networking (SDN) for large datacenter, cloud computing,\n",
      "high-performance computing, and high-frequency trading environments. These products include\n",
      "10/25/40/50/100/200/400/800 gigabit low-latency cut-through Ethernet switches. Arista's\n",
      "Linux-based network operating system, Extensible Operating System (EOS), runs on all Arista\n",
      "products.\n",
      "In 2004, Andy Bechtolsheim, Kenneth Duda and David Cheriton founded Arastra (later renamed\n",
      "Arista[3]). Bechtolsheim and Cheriton were able to fund the company themselves.[4] In May 2008,\n",
      "Jayshree Ullal left Cisco after 15 years at the firm. She was appointed CEO of Arista in October\n",
      "2008.[5]\n",
      "In June 2014, Arista Networks had its initial public offering on the New York Stock Exchange under\n",
      "the symbol ANET.[6]\n",
      "In December 2014, Cisco filed two lawsuits against Arista alleging intellectual property\n",
      "infringement.,[7] and the United States International Trade Commission issued limited exclusion and\n",
      "cease-and-desist orders concerning two of the features patented by Cisco[8] and upheld an import\n",
      "ban on infringing products.[9] In 2016, on appeal, the ban was reversed following product changes\n",
      "and two overturned Cisco patents, and Cisco's claim was dismissed.[10][11] In August 2018, Arista\n",
      "agreed to pay Cisco US$400 million as part of a settlement that included a release for all claims of\n",
      "infringement by Cisco, dismissal of Arista's antitrust claims against Cisco, and a 5-year stand-down\n",
      "between the companies.[12]\n",
      "In August 2018, Arista Networks acquired Mojo Networks.[13] In September 2018, Arista Networks\n",
      "acquired Metamako and integrated their low latency product line as the 7130 series.[14] In February\n",
      "2020, Arista acquired Big Switch Networks.[15] In October 2020, Arista acquired Awake\n",
      "Security.[16]\n",
      "Arista's CEO, Jayshree Ullal, was named to Barron's list of World's Best CEOs in 2018 and\n",
      "2019.[17]\n",
      "\n",
      "\n",
      "<|user|>\n",
      "What is AES?\n",
      "\n",
      "<|assistant|>\n",
      "AES, or the AES Corporation, is an American utility and power generation company that generates and sells electricity to end users and intermediaries like utilities and industrial facilities. It was founded in 1981 as Applied Energy Services and went public in 1991. The company has operations in 15 countries and employs over 10,500 people worldwide. AES has a history of self-management and social responsibility, with founders Roger Sant and Dennis Bakke implementing decentralized management and prioritizing social responsibility over profit. In recent years, AES has committed to providing renewable forms of energy, such as solar and wind-based energy storage systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 4: Query the vectorstore for relevant context\n",
    "def get_relevant_context(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform similarity search to retrieve the most relevant chunks from the vectorstore.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(question, k=top_k)\n",
    "    context = \"\\n\\n\".join([result.page_content for result in results])\n",
    "    return context\n",
    "\n",
    "# Step 5: Integrate everything for answering questions\n",
    "def ask_question(question):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from the vectorstore and use the LLMChain to answer the question.\n",
    "    \"\"\"\n",
    "    context = get_relevant_context(question)\n",
    "    result = llm_chain.run({\"context\": context, \"question\": question})\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "question = \"What is AES?\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_schema(connection):\n",
    "    \"\"\"\n",
    "    Extract the database schema as a string.\n",
    "    \"\"\"\n",
    "    schema = []\n",
    "    cursor = connection.cursor()\n",
    "    for table in cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall():\n",
    "        table_name = table[0]\n",
    "        schema.append(f\"Table: {table_name}\")\n",
    "        columns = cursor.execute(f\"PRAGMA table_info({table_name});\").fetchall()\n",
    "        for col in columns:\n",
    "            schema.append(f\"    Column: {col[1]} ({col[2]})\")\n",
    "    return \"\\n\".join(schema)\n",
    "\n",
    "schema = get_db_schema(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_schema_prompt = \"\"\"\n",
    "<|system|>\n",
    "You are an assistant that translates user questions into SQL queries for an SQLite database. \n",
    "The database schema is as follows:\n",
    "Price database: contains all prices from SP500 companies.\n",
    "Income statements: contains financial information from SP500 companies.\n",
    "\n",
    "{schema}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "\n",
    "<|assistant|>\n",
    "Here is the corresponding SQL query:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sql(question):\n",
    "    \"\"\"\n",
    "    Generate and execute a SQL query for the given question.\n",
    "    \"\"\"\n",
    "    # Prepare the inputs for the LLMChain\n",
    "    inputs = {\n",
    "        \"context\": schema,  # Provide the database schema as context\n",
    "        \"question\": question\n",
    "    }\n",
    "    \n",
    "    # Generate the SQL query\n",
    "    sql_query = llm_chain.run(inputs)\n",
    "    \n",
    "    # Execute the SQL query on the database\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql_query)\n",
    "        result = cursor.fetchall()\n",
    "        return {\"query\": sql_query, \"result\": result}\n",
    "    except sqlite3.Error as e:\n",
    "        return {\"query\": sql_query, \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\feder\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query:\n",
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "Table: income_statements\n",
      "    Column: date (TEXT)\n",
      "    Column: symbol (TEXT)\n",
      "    Column: reportedCurrency (TEXT)\n",
      "    Column: cik (TEXT)\n",
      "    Column: fillingDate (TEXT)\n",
      "    Column: acceptedDate (TEXT)\n",
      "    Column: calendarYear (TEXT)\n",
      "    Column: period (TEXT)\n",
      "    Column: revenue (INTEGER)\n",
      "    Column: costOfRevenue (INTEGER)\n",
      "    Column: grossProfit (INTEGER)\n",
      "    Column: grossProfitRatio (REAL)\n",
      "    Column: researchAndDevelopmentExpenses (INTEGER)\n",
      "    Column: generalAndAdministrativeExpenses (INTEGER)\n",
      "    Column: sellingAndMarketingExpenses (INTEGER)\n",
      "    Column: sellingGeneralAndAdministrativeExpenses (INTEGER)\n",
      "    Column: otherExpenses (INTEGER)\n",
      "    Column: operatingExpenses (INTEGER)\n",
      "    Column: costAndExpenses (INTEGER)\n",
      "    Column: interestIncome (INTEGER)\n",
      "    Column: interestExpense (INTEGER)\n",
      "    Column: depreciationAndAmortization (INTEGER)\n",
      "    Column: ebitda (INTEGER)\n",
      "    Column: ebitdaratio (REAL)\n",
      "    Column: operatingIncome (INTEGER)\n",
      "    Column: operatingIncomeRatio (REAL)\n",
      "    Column: totalOtherIncomeExpensesNet (INTEGER)\n",
      "    Column: incomeBeforeTax (INTEGER)\n",
      "    Column: incomeBeforeTaxRatio (REAL)\n",
      "    Column: incomeTaxExpense (INTEGER)\n",
      "    Column: netIncome (INTEGER)\n",
      "    Column: netIncomeRatio (REAL)\n",
      "    Column: eps (REAL)\n",
      "    Column: epsdiluted (REAL)\n",
      "    Column: weightedAverageShsOut (INTEGER)\n",
      "    Column: weightedAverageShsOutDil (INTEGER)\n",
      "    Column: link (TEXT)\n",
      "    Column: finalLink (TEXT)\n",
      "Table: price_data\n",
      "    Column: date (TIMESTAMP)\n",
      "    Column: ticker (TEXT)\n",
      "    Column: close (REAL)\n",
      "    Column: high (REAL)\n",
      "    Column: low (REAL)\n",
      "    Column: open (REAL)\n",
      "    Column: volume (REAL)\n",
      "    Column: garman_klass_vol (REAL)\n",
      "    Column: rsi (REAL)\n",
      "    Column: bb_low (REAL)\n",
      "    Column: bb_mid (REAL)\n",
      "    Column: bb_high (REAL)\n",
      "    Column: sharpe_ratio (REAL)\n",
      "    Column: atr (REAL)\n",
      "    Column: macd (REAL)\n",
      "    Column: dollar_volume (REAL)\n",
      "    Column: return_1d (REAL)\n",
      "    Column: return_2d (REAL)\n",
      "    Column: return_3d (REAL)\n",
      "    Column: return_6d (REAL)\n",
      "    Column: return_9d (REAL)\n",
      "    Column: return_12d (REAL)\n",
      "    Column: Mkt-RF (REAL)\n",
      "    Column: SMB (REAL)\n",
      "    Column: HML (REAL)\n",
      "    Column: RMW (REAL)\n",
      "    Column: CMA (REAL)\n",
      "Table: master_ticker\n",
      "    Column: index (INTEGER)\n",
      "    Column: ticker (TEXT)\n",
      "    Column: Security (TEXT)\n",
      "    Column: GICS Sector (TEXT)\n",
      "    Column: GICS Sub-Industry (TEXT)\n",
      "    Column: Headquarters Location (TEXT)\n",
      "    Column: Date added (TEXT)\n",
      "    Column: CIK (INTEGER)\n",
      "    Column: Founded (TEXT)\n",
      "\n",
      "\n",
      "<|user|>\n",
      "List all securities with a market value greater than $1,000,000.\n",
      "\n",
      "<|assistant|>\n",
      "To list all securities with a market value greater than $1,000,000 using this database schema, you would need to join the income_statements and price_data tables to calculate the market value for each security, and then filter the results based on the market value threshold. Here's an example SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT DISTINCT m.Security, p.close * p.volume AS marketValue\n",
      "FROM (\n",
      "    SELECT DISTINCT ticker\n",
      "    FROM income_statements\n",
      "    WHERE YEAR(date) = YEAR(CURDATE()) AND MONTH(date) = MONTH(CURDATE())\n",
      ") AS i\n",
      "JOIN (\n",
      "    SELECT DISTINCT ticker\n",
      "    FROM price_data\n",
      "    WHERE date = (\n",
      "        SELECT MAX(date)\n",
      "        FROM price_data\n",
      "        WHERE ticker = price_data.ticker\n",
      "    )\n",
      ") AS p ON i.ticker = p.ticker\n",
      "JOIN master_ticker AS m ON i.ticker = m.ticker\n",
      "WHERE marketValue > 1000000\n",
      "ORDER BY marketValue DESC;\n",
      "```\n",
      "\n",
      "This query first selects all unique tickers from the income_statements table that have reports in the current year and month. It then joins this list of tickers with the price_data table to get the most recent closing price and volume for each security. The market value is calculated by multiplying the closing price and volume together. Finally, it joins this list of tickers with the master_ticker table to get the full name of each security. The results are filtered to only include securities with a market value greater than $1,000,000, and sorted by descending market value.\n",
      "Error executing query: near \"<\": syntax error\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"List all securities with a market value greater than $1,000,000.\"\n",
    "output = text_to_sql(question)\n",
    "\n",
    "print(\"Generated SQL Query:\")\n",
    "print(output[\"query\"])\n",
    "\n",
    "if \"error\" in output:\n",
    "    print(\"Error executing query:\", output[\"error\"])\n",
    "else:\n",
    "    print(\"Query Result:\")\n",
    "    for row in output[\"result\"]:\n",
    "        print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
