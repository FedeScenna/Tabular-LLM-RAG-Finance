{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Newton's second law of motion\"},\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PDFRAGSystem:\n",
    "    def __init__(self, model_name: str = \"llama2\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with specified Ollama model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = OllamaEmbeddings(model=model_name)\n",
    "        self.llm = Ollama(model=model_name)\n",
    "        self.vector_store = None\n",
    "        \n",
    "    def load_pdfs(self, pdf_directory: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Load all PDFs from the specified directory.\n",
    "        \n",
    "        Args:\n",
    "            pdf_directory (str): Path to directory containing PDF files\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of extracted text from PDFs\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        for filename in os.listdir(pdf_directory):\n",
    "            if filename.endswith('.pdf'):\n",
    "                file_path = os.path.join(pdf_directory, filename)\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents.extend(loader.load())\n",
    "                \n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        return text_splitter.split_documents(documents)\n",
    "    \n",
    "    def create_vector_store(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Create FAISS vector store from documents.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): List of document chunks\n",
    "        \"\"\"\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents,\n",
    "            self.embeddings\n",
    "        )\n",
    "    \n",
    "    def setup_qa_chain(self) -> RetrievalQA:\n",
    "        \"\"\"\n",
    "        Set up the question-answering chain.\n",
    "        \n",
    "        Returns:\n",
    "            RetrievalQA: The QA chain object\n",
    "        \"\"\"\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Question to ask about the PDFs\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Response containing answer and source documents\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"Vector store not initialized. Please load documents first.\")\n",
    "            \n",
    "        qa_chain = self.setup_qa_chain()\n",
    "        return qa_chain({\"query\": question})\n",
    "\n",
    "def main():\n",
    "    # Initialize the RAG system\n",
    "    rag_system = PDFRAGSystem(model_name=\"llama2\")\n",
    "    \n",
    "    # Load PDFs from directory\n",
    "    pdf_dir = \"path/to/your/pdfs\"\n",
    "    documents = rag_system.load_pdfs(pdf_dir)\n",
    "    \n",
    "    # Create vector store\n",
    "    rag_system.create_vector_store(documents)\n",
    "    \n",
    "    # Example query\n",
    "    question = \"What are the main topics discussed in these documents?\"\n",
    "    response = rag_system.query(question)\n",
    "    \n",
    "    print(\"Answer:\", response[\"result\"])\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in response[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata['source']}, page {doc.metadata['page']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
